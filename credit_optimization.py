# -*- coding: utf-8 -*-
"""Task 4 Chase.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rwCW9RjBdMWf0ggzvVabcYZWTWGhBffh
"""

import numpy as np
import pandas as pd
from typing import List, Tuple


def calculate_log_likelihood(n: int, k: int) -> float:
    if n == 0 or k == 0 or k == n:
        return 0.0

    p = k / n
    ll = k * np.log(p) + (n - k) * np.log(1 - p)
    return ll


def find_optimal_buckets_fast(scores: np.ndarray, defaults: np.ndarray, num_buckets: int) -> Tuple[List[float], dict]:
    data = pd.DataFrame({'score': scores, 'default': defaults})
    data = data.sort_values('score').reset_index(drop=True)

    n = len(data)
    scores_sorted = data['score'].values
    defaults_sorted = data['default'].values

    max_sample = 2000
    if n > max_sample:
        step = n // max_sample
        sample_indices = np.arange(0, n, step)
        scores_sample = scores_sorted[sample_indices]
        defaults_sample = defaults_sorted[sample_indices]
        n_sample = len(sample_indices)
    else:
        scores_sample = scores_sorted
        defaults_sample = defaults_sorted
        n_sample = n
        sample_indices = np.arange(n)

    cumsum_n = np.arange(1, n_sample + 1)
    cumsum_k = np.cumsum(defaults_sample)

    dp = np.full((n_sample, num_buckets), -np.inf)
    backtrack = np.zeros((n_sample, num_buckets), dtype=int)

    for i in range(n_sample):
        seg_n = cumsum_n[i]
        seg_k = cumsum_k[i]
        dp[i, 0] = calculate_log_likelihood(seg_n, seg_k)

    for b in range(1, num_buckets):
        for i in range(b, n_sample):
            for j in range(b-1, i):
                seg_n = cumsum_n[i] - cumsum_n[j]
                seg_k = cumsum_k[i] - cumsum_k[j]
                ll_current = calculate_log_likelihood(seg_n, seg_k)
                total_ll = dp[j, b-1] + ll_current

                if total_ll > dp[i, b]:
                    dp[i, b] = total_ll
                    backtrack[i, b] = j

    split_indices = []
    current = n_sample - 1
    for b in range(num_buckets - 1, 0, -1):
        prev = backtrack[current, b]
        split_indices.insert(0, sample_indices[prev + 1])
        current = prev

    boundaries = [scores_sorted[0]]
    for idx in split_indices:
        boundaries.append(scores_sorted[idx])

    rating_map = {}
    prev_idx = 0
    for rating, next_idx in enumerate(split_indices + [n]):
        segment_scores = scores_sorted[prev_idx:next_idx]
        segment_defaults = defaults_sorted[prev_idx:next_idx]

        seg_n = len(segment_scores)
        seg_k = np.sum(segment_defaults)
        default_rate = seg_k / seg_n if seg_n > 0 else 0

        rating_map[rating] = {
            'score_range': (segment_scores[0], segment_scores[-1]),
            'count': seg_n,
            'defaults': int(seg_k),
            'default_rate': default_rate
        }

        prev_idx = next_idx

    return boundaries, rating_map


if __name__ == "__main__":
    df = pd.read_csv('Task 3 and 4_Loan_Data.csv')

    scores = df['fico_score'].values
    defaults = df['default'].values

    num_buckets = 5

    print("Processing...")
    boundaries, rating_map = find_optimal_buckets_fast(scores, defaults, num_buckets)

    print(f"\nOptimal Rating Map with {num_buckets} buckets:")
    print(f"{'Rating':<10} {'Score Range':<20} {'Count':<10} {'Defaults':<10} {'Default Rate':<15}")
    print("-" * 75)

    for rating in sorted(rating_map.keys()):
        info = rating_map[rating]
        score_range = f"{info['score_range'][0]:.0f} - {info['score_range'][1]:.0f}"
        print(f"{rating:<10} {score_range:<20} {info['count']:<10} {info['defaults']:<10} {info['default_rate']:<15.4f}")